{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Supress Warning \n",
    "import warnings\n",
    "\n",
    "# sklearn metrics and scaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Custom DL models\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "from model import custom\n",
    "from utils.callback import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# minus sign\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# set save_path to bring data and save result\n",
    "# we will save processed data here\n",
    "save_path = './result/'\n",
    "# # save results\n",
    "# os.mkdir(save_path)\n",
    "\n",
    "# ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import custom models\n",
    "importlib.reload(custom)\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# columns of temp, export, consum\n",
    "with open(save_path+'names.txt', 'r') as file:\n",
    "    names = file.read()\n",
    "names = json.loads(names)\n",
    "\n",
    "# dictionary to store evaluation\n",
    "result = defaultdict(dict)\n",
    "\n",
    "# Dataset\n",
    "consum = pd.read_csv(save_path+'original.csv', encoding='utf-8-sig')\n",
    "\n",
    "# Functions\n",
    "\n",
    "## Data Slicing\n",
    "def slicing(data):\n",
    "    n = len(data)\n",
    "    train_df = data[0:int(n*0.7), :]\n",
    "    test_df = data[int(n*0.7):, :]\n",
    "    return train_df, test_df\n",
    "\n",
    "## Visualize Loss\n",
    "def his(data, name, location):\n",
    "    plt.plot(data.history['loss'], label='train')\n",
    "    plt.plot(data.history['val_loss'], label='val')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"====\"+location+\"====\"+name+\"====\")\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + 'uni' + location + name + '_loss.png', dpi=700)\n",
    "    plt.show()\n",
    "\n",
    "## Evaluation Metrics\n",
    "RMSE = lambda x, y: mean_squared_error(x, y, squared=False)\n",
    "MAPE = mean_absolute_percentage_error\n",
    "\n",
    "def evaluation(label, outputs, label_width):\n",
    "\n",
    "    return {\n",
    "        \"RMSE\": [RMSE(label[:, i], outputs[:, i]) for i in range(label_width)],\n",
    "        \"MAPE\": [MAPE(label[:, i], outputs[:, i]) for i in range(label_width)],\n",
    "    }\n",
    "\n",
    "## nested Dict to Dataframe\n",
    "def dict2df(dict):\n",
    "    reformed_dict = {}\n",
    "    for outerKey, innerDict in dict.items():\n",
    "        for innerKey, values in innerDict.items():\n",
    "            reformed_dict[(outerKey, innerKey)] = values\n",
    "    dict = pd.DataFrame(reformed_dict)\n",
    "    return dict \n",
    "\n",
    "## Modeling DL Algorithms \n",
    "def Modeling(dataset, model_name, config, label, location):  \n",
    "    # config\n",
    "    input_width = config[\"input_width\"]\n",
    "    label_width = config[\"label_width\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    vb = config[\"vb\"].copy()\n",
    "    units = config[\"units\"]\n",
    "    drop = config[\"drop\"]\n",
    "\n",
    "    n_features = 1\n",
    "\n",
    "    dataset = dataset[(dataset[\"region\"]==location) & (dataset[\"contract\"]==label)]\n",
    "    dataset = dataset[[\"period\", \"usage(kWh)\"]]\n",
    "\n",
    "    # TimeIndex\n",
    "    dataset['period'] = pd.to_datetime(dataset['period'])\n",
    "    dataset = dataset.set_index('period')\n",
    "    values = dataset.values.astype('float32')\n",
    "\n",
    "    # Data Slicing\n",
    "    train_set, test_set = slicing(data=values)\n",
    "\n",
    "    # Normalization \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_set = scaler.fit_transform(train_set)\n",
    "    test_set = scaler.transform(test_set)\n",
    "\n",
    "    # reshape into X=t and Y=t+timestep\n",
    "    train_set = custom.generator(train_set, input_width, label_width)\n",
    "    test_set = custom.generator(test_set, input_width, label_width)\n",
    "    train_set = train_set.values\n",
    "    test_set = test_set.values\n",
    "\n",
    "    # split into input and outputs\n",
    "    n_obs = input_width * n_features\n",
    "    trainX, trainY = train_set[:, :n_obs], train_set[:, -n_features]\n",
    "    testX, testY = test_set[:, :n_obs], test_set[:, -n_features]\n",
    "    \n",
    "    # reshape output to be [samples, timesteps (lag), features]\n",
    "    trainX = trainX.reshape((trainX.shape[0], input_width, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], input_width, n_features))\n",
    "\n",
    "    # create and fit model\n",
    "    models = custom.get_model(model_name, units=units, input_width = input_width, \n",
    "                            label_width = label_width, n_features=n_features, dropout = drop)\n",
    "    history = models.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, \n",
    "                    validation_data = (testX, testY), verbose=vb[0], \n",
    "                    # callbacks=[\n",
    "                    #     EarlyStopping(patience=30, verbose=vb[0], min_epoch=100),\n",
    "                    #     ModelCheckpoint(f\"{save_path}_uni_{model_name}_{location}.h5\", verbose=vb[0], min_epoch=100,\n",
    "                    #     save_best_only=True, save_weights_only=True)],\n",
    "                    shuffle=False)\n",
    "\n",
    "    # # model summary\n",
    "    # print(models.summary())\n",
    "    \n",
    "    # # plot history\n",
    "    # his(history, model_name, location)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = models.predict(trainX)\n",
    "    testPredict = models.predict(testX)\n",
    "\n",
    "    trainY = trainY.reshape((len(trainY), 1))\n",
    "    testY = testY.reshape((len(testY), 1))\n",
    "\n",
    "    ## Inverse Transform\n",
    "    def inverse(Predict, X, Y):\n",
    "        # reshape for inverse scaling\n",
    "        X = X.reshape((X.shape[0], input_width*n_features)) \n",
    "        if model_name == \"linear\":\n",
    "            prd = np.concatenate((Predict[:, 0, :], X[:, -(n_features-1):]), axis=1)\n",
    "        else:\n",
    "            prd = np.concatenate((Predict[:, :, 0], X[:, -(n_features-1):]), axis=1)\n",
    "        trg = np.concatenate((Y, X[:, -(n_features-1):]), axis=1)\n",
    "        # inverse scaling prediction\n",
    "        inverse_prd = scaler.inverse_transform(prd)\n",
    "        # inverse scaling target\n",
    "        inverse_trg = scaler.inverse_transform(trg)\n",
    "        return inverse_prd, inverse_trg\n",
    "            \n",
    "    train_prd, train_trg = inverse(trainPredict, trainX, trainY)\n",
    "    test_prd, test_trg = inverse(testPredict, testX, testY)\n",
    "\n",
    "    # calculate evaluation metrics\n",
    "    testEval = evaluation(testY, testPredict[:, :, 0], label_width=label_width)\n",
    "\n",
    "    # # shift train predictions for plotting\n",
    "    # trainPlot = np.empty_like(dataset)\n",
    "    # trainPlot[:, :] = np.nan\n",
    "    # trainPlot[input_width:len(train_prd)+input_width, 0] = train_prd[:, 0]\n",
    "    # # shift test predictions for plotting\n",
    "    # testPlot = np.empty_like(dataset)\n",
    "    # testPlot[:, :] = np.nan\n",
    "    # testPlot[len(train_prd)+input_width:len(train_prd)+len(test_prd)+input_width, 0] = test_prd[:, 0]\n",
    "    # # unshifted baseline\n",
    "    # baseline  = np.empty_like(dataset)\n",
    "    # baseline[:, :] = np.nan\n",
    "    # baseline[:len(train_trg), 0] = train_trg[:, 0]\n",
    "    # baseline[len(train_trg):len(train_trg)+len(test_trg), 0] = test_trg[:, 0]\n",
    "    # # plot baseline and predictions\n",
    "    # plt.plot(baseline)\n",
    "    # plt.plot(trainPlot)\n",
    "    # plt.plot(testPlot)  \n",
    "    # plt.title('uni' + model_name+location+'_trg_prd')\n",
    "    # plt.savefig(f'{save_path}uni_{model_name}_{location}_trg_prd.png', dpi=500)\n",
    "    # plt.show()\n",
    "    \n",
    "    result[model_name][location] = {}\n",
    "    result[model_name][location]['eval'] = testEval\n",
    "    result[model_name][location]['targ'] = test_trg\n",
    "    result[model_name][location]['prds'] = test_prd\n",
    "\n",
    "    # print(location)\n",
    "    # print(result[model_name][location]['eval']['RMSE'])\n",
    "    # print(result[model_name][location]['eval']['MAPE'])\n",
    "    return result\n",
    "\n",
    "## Main\n",
    "def main(dataset, model_name, label, location=\"all\", config=None):\n",
    "    # Nationwide data\n",
    "    exm = dataset.copy()\n",
    "    bysum = exm[names['consum']].copy()\n",
    "    bysum = bysum.groupby(['period', 'contract']).sum().reset_index()\n",
    "    bysum[\"region\"] = \"KOR\"\n",
    "    dataset  = pd.concat([exm, bysum], ignore_index=True)\n",
    "    dataset = dataset[dataset[\"region\"] != 'Sejong']\n",
    "    # state-level \n",
    "    state = list(set(dataset[\"region\"])-set([\"KOR\"]))\n",
    "    if location==\"all\":\n",
    "        for i in state:\n",
    "            result = Modeling(dataset = dataset, model_name=model_name, \n",
    "                            config=config, label=label, location=i)\n",
    "    # nation-level\n",
    "    else:\n",
    "        result = Modeling(dataset = dataset, model_name=model_name, \n",
    "                        config=config, label=label, location=location)\n",
    "\n",
    "    result = dict2df(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single-step model\n",
    "np.random.seed(123) \n",
    "tf.random.set_seed(123) \n",
    "\n",
    "# learning_rate = 1e-5\n",
    "# decay = None\n",
    "\n",
    "units = 128 # 128 dimension\n",
    "input_width = 2 # input sequence (number of lag months)\n",
    "label_width = 1 # forecast sequence (number of output months)\n",
    "batch_size = 16\n",
    "epochs = 140 \n",
    "drop = 0.1\n",
    "vb = [0, 1, 2]\n",
    "\n",
    "config = {'units': units, 'input_width': input_width, \n",
    "        'label_width': label_width, 'batch_size': batch_size, \n",
    "        'epochs': epochs, 'drop': drop, 'vb': vb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## National/State-level\n",
    "n_lin = main(dataset=consum, model_name=\"linear\", label= \"total\", location=\"KOR\", config=config)\n",
    "all_lin = main(dataset=consum, model_name=\"linear\", label= \"total\", location=\"all\", config=config)\n",
    "n_rnn =  main(dataset=consum, model_name=\"rnn\", label= \"total\", location=\"KOR\", config=config) \n",
    "all_rnn = main(dataset=consum, model_name=\"rnn\", label= \"total\", location=\"all\", config=config)\n",
    "n_lstm = main(dataset=consum, model_name=\"lstm\", label= \"total\", location=\"KOR\", config=config) \n",
    "all_lstm = main(dataset=consum, model_name=\"lstm\", label= \"total\", location=\"all\", config=config)\n",
    "n_cnnlstm = main(dataset=consum, model_name=\"cnn-lstm\", label= \"total\", location=\"KOR\", config=config)\n",
    "all_cnnlstm = main(dataset=consum, model_name=\"cnn-lstm\", label= \"total\", location=\"all\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cnnlstm.to_csv(save_path+'uni_result.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d34db905673ab59baec526eee802ee4590a4f3f0c3e34c71e17a6029dd2c9208"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('bigdata': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
