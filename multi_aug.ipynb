{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariable Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Supress Warning \n",
    "import warnings\n",
    "\n",
    "# sklearn metrics and scaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Custom DL models\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "from model import custom\n",
    "from utils.callback import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GPU memory setup to prevent crush\n",
    "# gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "\n",
    "%matplotlib inline\n",
    "# minus sign\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# set save_path to bring data and save result\n",
    "# we will save processed data here\n",
    "save_path = './result/'\n",
    "# # save results\n",
    "# os.mkdir(save_path)\n",
    "\n",
    "# ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import custom models\n",
    "importlib.reload(custom)\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# columns of temp, export, consum\n",
    "with open(save_path+'names.txt', 'r') as file:\n",
    "    names = file.read()\n",
    "names = json.loads(names)\n",
    "\n",
    "# dictionary to store results\n",
    "result = defaultdict(dict)\n",
    "\n",
    "# Dataset\n",
    "total = pd.read_csv(save_path+'preprocessed.csv', encoding='utf-8-sig')\n",
    "consum = pd.read_csv(save_path+'original.csv', encoding='utf-8-sig')\n",
    "\n",
    "# Functions\n",
    "\n",
    "## Data Augmentation\n",
    "def aug(data):\n",
    "    d = data.copy()\n",
    "    col_name = list(set(d.columns)-set(['period']))\n",
    "    d[col_name] = d[col_name].values.astype('float32')\n",
    "    d['period'] = pd.to_datetime(d['period'])\n",
    "    # dataframe to series\n",
    "    d = d.squeeze()\n",
    "    # TimeIndex\n",
    "    d = d.set_index('period')\n",
    "    # upsampling by Day\n",
    "    upsampled = d.resample('D')\n",
    "    # fillna with Spline interpolation\n",
    "    interpolated = upsampled.interpolate(method='spline', order=2)\n",
    "    interpolated = interpolated.dropna()\n",
    "    return interpolated\n",
    "\n",
    "## Apply Data Augmentation\n",
    "def get_data(data):\n",
    "    # Nationwide data\n",
    "    temp = data.copy()\n",
    "    # state-level \n",
    "    state = list(set(temp[\"region\"]))\n",
    "    ## Data Augmentation\n",
    "    emp = []\n",
    "    for i in state:\n",
    "        df = temp[(temp[\"region\"]==i) & (temp[\"contract\"]=='total')]\n",
    "        df = df[df.columns[~df.columns.isin([\"Unnamed: 0\", \"region\", \"contract\",'Unnamed: 0_x', 'Unnamed: 0_y'])]]\n",
    "        df = aug(df.copy())\n",
    "        df = df.reset_index()\n",
    "        df['region'] = i\n",
    "        df['contract'] = 'total'\n",
    "        emp.append(df)\n",
    "    return pd.concat(emp, ignore_index=True)\n",
    "\n",
    "## Data Slicing\n",
    "def slicing(data):\n",
    "    n = len(data)\n",
    "    train_df = data[0:int(n*0.7), :]\n",
    "    test_df = data[int(n*0.7):, :]\n",
    "    return train_df, test_df\n",
    "\n",
    "## Visualize Loss\n",
    "def his(data, name, location):\n",
    "    plt.plot(data.history['loss'], label='train')\n",
    "    plt.plot(data.history['val_loss'], label='val')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"====\"+location+\"====\"+name+\"====\")\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path + 'multi_aug' + location + name + '_loss.png', dpi=700)\n",
    "    plt.show()\n",
    "\n",
    "## Evaluation Metrics\n",
    "RMSE = lambda x, y: mean_squared_error(x, y, squared=False)\n",
    "MAPE = mean_absolute_percentage_error\n",
    "\n",
    "def evaluation(label, outputs, label_width):\n",
    "    return {\n",
    "        \"RMSE\": [RMSE(label[:, i], outputs[:, i]) for i in range(label_width)],\n",
    "        \"MAPE\": [MAPE(label[:, i], outputs[:, i]) for i in range(label_width)],\n",
    "    }\n",
    "\n",
    "## Extract Forecasting for Original dataset (i.e. monthly dataset)\n",
    "def extract(data, augmented, input_width, train=False):\n",
    "    df = pd.DataFrame(data)\n",
    "    augmented = augmented.reset_index()\n",
    "    months = consum['period'].values\n",
    "    daily = augmented['period'].copy()\n",
    "    n = len(daily)\n",
    "    if train:\n",
    "        df['period'] = daily[input_width:int(n*0.7)].values\n",
    "    else:\n",
    "        df['period'] = daily[int(n*0.7)+input_width:].values\n",
    "    df = df.loc[df['period'].isin(months)]\n",
    "    df = df.drop(columns=['period'])\n",
    "    return df.values.astype('float32')\n",
    "\n",
    "## nested Dict to Dataframe\n",
    "def dict2df(dict):\n",
    "    reformed_dict = {}\n",
    "    for outerKey, innerDict in dict.items():\n",
    "        for innerKey, values in innerDict.items():\n",
    "            reformed_dict[(outerKey, innerKey)] = values\n",
    "    dict = pd.DataFrame(reformed_dict)\n",
    "    return dict \n",
    "\n",
    "## Modeling DL Algorithms \n",
    "def Modeling(dataset, model_name, config, label, location):  \n",
    "    # config\n",
    "    input_width = config[\"input_width\"]\n",
    "    label_width = config[\"label_width\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    vb = config[\"vb\"]\n",
    "    units = config[\"units\"]\n",
    "    drop = config[\"drop\"]\n",
    "    \n",
    "    n_features = 3\n",
    "\n",
    "    dataset = dataset[(dataset[\"region\"]==location) & (dataset[\"contract\"]==label)]\n",
    "    dataset = dataset[[\"period\", \"usage(kWh)\", \"export_value\", \"import_value\"]]\n",
    "    col_name = list(set(dataset.columns)-set(['period']))\n",
    "    dataset[col_name] = dataset[col_name].values.astype('float32')\n",
    "    dataset['period'] = pd.to_datetime(dataset['period'])\n",
    "\n",
    "    # TimeIndex\n",
    "    dataset = dataset.set_index('period')\n",
    "    values = dataset.values\n",
    "\n",
    "    # Data Slicing\n",
    "    train_set, test_set = slicing(data=values)\n",
    "\n",
    "    # Normalization \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_set = scaler.fit_transform(train_set)\n",
    "    test_set = scaler.transform(test_set)\n",
    "\n",
    "    # reshape into X=t and Y=t+timestep\n",
    "    train_set = custom.generator(train_set, input_width, label_width)\n",
    "    test_set = custom.generator(test_set, input_width, label_width)\n",
    " \n",
    "    train_set = train_set.values\n",
    "    test_set = test_set.values\n",
    "\n",
    "    # split into input and outputs\n",
    "    n_obs = input_width * n_features\n",
    "    trainX, trainY = train_set[:, :n_obs], train_set[:, -n_features]\n",
    "    testX, testY = test_set[:, :n_obs], test_set[:, -n_features]\n",
    "    \n",
    "    # reshape output to be [samples, timesteps (lag), features]\n",
    "    trainX = trainX.reshape((trainX.shape[0], input_width, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], input_width, n_features))\n",
    "\n",
    "    # create and fit model\n",
    "    models = custom.get_model(model_name, units=units, input_width = input_width, \n",
    "                            label_width = label_width, n_features=n_features, dropout = drop)\n",
    "    history = models.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, \n",
    "                    validation_data = (testX, testY), verbose=vb[0], \n",
    "                    # callbacks=[\n",
    "                    #     EarlyStopping(patience=30, verbose=vb[0], min_epoch=100),\n",
    "                    #     ModelCheckpoint(f\"{save_path}_multi_aug_{model_name}_{location}.h5\", verbose=vb[0], min_epoch=100,\n",
    "                    #     save_best_only=True, save_weights_only=True)],\n",
    "                    shuffle=False)\n",
    "\n",
    "    # # model summary\n",
    "    # print(models.summary())\n",
    "    \n",
    "    # # plot history\n",
    "    # his(history, model_name, location)\n",
    "\n",
    "    # make predictions\n",
    "    trainPredict = models.predict(trainX)\n",
    "    testPredict = models.predict(testX)\n",
    "\n",
    "    trainY = trainY.reshape((len(trainY), 1))\n",
    "    testY = testY.reshape((len(testY), 1))\n",
    "\n",
    "    ## Inverse Transform\n",
    "    def inverse(Predict, X, Y):\n",
    "        # reshape for inverse scaling\n",
    "        X = X.reshape((X.shape[0], input_width*n_features)) \n",
    "        if model_name == \"linear\":\n",
    "            prd = np.concatenate((Predict[:, 0, :], X[:, -(n_features-1):]), axis=1)\n",
    "        else:\n",
    "            prd = np.concatenate((Predict[:, :, 0], X[:, -(n_features-1):]), axis=1)\n",
    "        trg = np.concatenate((Y, X[:, -(n_features-1):]), axis=1)\n",
    "        # inverse scaling prediction\n",
    "        inverse_prd = scaler.inverse_transform(prd)\n",
    "        # inverse scaling target\n",
    "        inverse_trg = scaler.inverse_transform(trg)\n",
    "        return inverse_prd, inverse_trg\n",
    "            \n",
    "    train_prd, train_trg = inverse(trainPredict, trainX, trainY)\n",
    "    test_prd, test_trg = inverse(testPredict, testX, testY)\n",
    "\n",
    "    # calculate evaluation metrics\n",
    "    testEval = evaluation(testY, testPredict[:, :, 0], label_width=label_width)\n",
    "\n",
    "    # extract monthly result\n",
    "    train_prd = extract(train_prd, dataset, input_width, train=True)\n",
    "    test_prd = extract(test_prd, dataset, input_width)\n",
    "    train_trg = extract(train_trg, dataset, input_width, train=True)\n",
    "    test_trg = extract(test_trg, dataset, input_width)\n",
    "\n",
    "    # # shift train predictions for plotting\n",
    "    # trainPlot = np.empty_like(dataset)\n",
    "    # trainPlot[:, :] = np.nan\n",
    "    # trainPlot[input_width:len(train_prd)+input_width, 0] = train_prd[:, 0]\n",
    "    # # shift test predictions for plotting\n",
    "    # testPlot = np.empty_like(dataset)\n",
    "    # testPlot[:, :] = np.nan\n",
    "    # testPlot[len(train_prd)+input_width:len(train_prd)+len(test_prd)+input_width, 0] = test_prd[:, 0]\n",
    "    # # unshifted baseline\n",
    "    # baseline  = np.empty_like(dataset)\n",
    "    # baseline[:, :] = np.nan\n",
    "    # baseline[:len(train_trg), 0] = train_trg[:, 0]\n",
    "    # baseline[len(train_trg):len(train_trg)+len(test_trg), 0] = test_trg[:, 0]\n",
    "    # # plot baseline and predictions\n",
    "    # plt.plot(baseline)\n",
    "    # plt.plot(trainPlot)\n",
    "    # plt.plot(testPlot)  \n",
    "    # plt.title('multi_aug'+model_name+location+'_trg_prd')\n",
    "    # plt.savefig(f'{save_path}multi_aug_{model_name}_{location}_trg_prd.png', dpi=500)\n",
    "    # plt.show()\n",
    "\n",
    "    result[model_name][location] = {}\n",
    "    result[model_name][location]['eval'] = testEval\n",
    "    result[model_name][location]['targ'] = test_trg\n",
    "    result[model_name][location]['prds'] = test_prd\n",
    "\n",
    "    # print(location)\n",
    "    # print(result[model_name][location]['eval']['RMSE'])\n",
    "    # print(result[model_name][location]['eval']['MAPE'])\n",
    "    return result\n",
    "\n",
    "## Main\n",
    "def main(dataset, model_name, label, location=\"all\", config=None):\n",
    "    # Nationwide data\n",
    "    exm = dataset.copy()\n",
    "    bymean = exm[names['temp']].copy()\n",
    "    bymean = bymean.groupby(['period']).mean().reset_index()\n",
    "    bysum = exm[set(names['consum']+names['export'])].copy()\n",
    "    bysum = bysum.groupby(['period', 'contract']).sum().reset_index()\n",
    "    bysum[\"region\"] = \"KOR\"\n",
    "    bysum = bysum.merge(bymean, how='outer', on='period').reset_index(drop=True)\n",
    "    dataset  = pd.concat([exm, bysum], ignore_index=True)\n",
    "    dataset = dataset[dataset[\"region\"] != 'Sejong']\n",
    "    # state-level \n",
    "    state = list(set(dataset[\"region\"])-set([\"KOR\"]))\n",
    "    dataset = get_data(dataset)\n",
    "    if location==\"all\":\n",
    "        for i in state:\n",
    "            result = Modeling(dataset = dataset, model_name=model_name, \n",
    "                            config=config, label=label, location=i)\n",
    "    # nation-level\n",
    "    else:\n",
    "        result = Modeling(dataset = dataset, model_name=model_name, \n",
    "                        config=config, label=label, location=location)\n",
    "\n",
    "    result = dict2df(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single-step model\n",
    "np.random.seed(123)  #123\n",
    "tf.random.set_seed(123) #123\n",
    "\n",
    "# learning_rate = 1e-6\n",
    "# decay = None\n",
    "\n",
    "units = 128 # 128 dimension \n",
    "input_width = 30 # input sequence (number of lag days) \n",
    "label_width = 1 # forecast sequence (number of output days)\n",
    "batch_size = 16 #8\n",
    "epochs = 100 \n",
    "drop = 0.1\n",
    "vb = [0, 1, 2]\n",
    "\n",
    "config = {'units': units, 'input_width': input_width, \n",
    "        'label_width': label_width, 'batch_size': batch_size, \n",
    "        'epochs': epochs, 'drop': drop, 'vb': vb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## National/State-level\n",
    "# n_lin: national\n",
    "# all_lin: all states\n",
    "n_lin = main(dataset=total, model_name=\"linear\", label= \"total\", location=\"KOR\", config=config)\n",
    "all_lin = main(dataset=total, model_name=\"linear\", label= \"total\", location=\"all\", config=config)\n",
    "n_rnn =  main(dataset=total, model_name=\"rnn\", label= \"total\", location=\"KOR\", config=config)\n",
    "all_rnn = main(dataset=total, model_name=\"rnn\", label= \"total\", location=\"all\", config=config)\n",
    "n_lstm = main(dataset=total, model_name=\"lstm\", label= \"total\", location=\"KOR\", config=config)\n",
    "all_lstm = main(dataset=total, model_name=\"lstm\", label= \"total\", location=\"all\", config=config)\n",
    "n_cnnlstm = main(dataset=total, model_name=\"cnn-lstm\", label= \"total\", location=\"KOR\", config=config)\n",
    "all_cnnlstm = main(dataset=total, model_name=\"cnn-lstm\", label= \"total\", location=\"all\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to csv\n",
    "# all_cnnlstm has all results of previous training\n",
    "all_cnnlstm.to_csv(save_path+'multi_aug_result.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca3a6f1e70ac8c70ed94bce3f5714b9ff82547952b566d52ca0914aca51b6110"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
